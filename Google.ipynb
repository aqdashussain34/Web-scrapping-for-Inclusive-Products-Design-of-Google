{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63817e8f-f4f8-4270-a006-c5af0e3865a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted content saved to Formatted_Scraped_Content_Google.docx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to generate a summary? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " Designing Inclusive Products for Everyone — Google Belonging Jump to content Belonging Overview At work In products In products What we’re doing Accessibility in our products In society Key issues Key issues Disability inclusion Gender equity LGBTQ+ inclusion Racial equity Veteran inclusion Resources Resources Build inclusive products Build accessible technology Create inclusive marketing Diversity report About Google Our mission, products, and impact More about our core commitments Belonging Expanding what’s possible for everyone Learning Unlocking opportunity with education & career tools Safety Center Keeping billions of people safe online Crisis Response Helping people with information in critical moments Sustainability Committed to being carbon free by 2030 Diversity report Helpful technology enables everyone to pursue their goals. Building tools that see people fairly, the power that comes from that is that people dream to be the biggest versions of themselves that they can imagine.” Link copied to clipboard Photography by Devyn Galindo More ways we’re building belonging in our products: Expanding accessible learning with Google for Education Celebrate Native American artists in Chrome and ChromeOS “Lift as you lead”: Meet 2 women defining responsible AI 4 ways Google Assistant helps me manage sensory overload Expanding accessible learning with Google for Education Celebrate Native American artists in Chrome and ChromeOS “Lift as you lead”: Meet 2 women defining responsible AI 4 ways Google Assistant helps me manage sensory overload Read more belonging stories on our blog (Opens in a New Browser Tab) Explore more of our belonging work We’re sharing tools to help creative technologists build truly accessible websites and apps. Learn more (Opens in a New Browser Tab) Watch the film 2:49 Close dialog window How we worked with the disability community to improve our speech recognition technology Watch the film 2:54 Close dialog window How our employee resource groups brought an inclusive lens to a core product Help us improve our products by getting involved in UX research (Opens in a New Browser Tab) Google employee’s quote Unmute Mute Play audio and video Play video (audio currently muted) Pause Dimitri Kanevsky (he/him) Research Scientist, Google Read the transcript (Opens in a Dialog Window) Close dialog window “I was born in Russia. Features to help people identify and support inclusive spaces on Google Maps and Search Learn about the business attributes (Opens in a Dialog Window) Close dialog window Business attributes to help people identify and support inclusive spaces on Google Maps and Search Link copied to clipboard Faye Orlove (she/her) (center), founder of Junior High, a community arts and event space that promotes the work of marginalized and underrepresented artists We’ve added features to Google Maps and Search that help people find the spaces they need, and support diversely owned businesses. Features to help people identify and support inclusive spaces on Google Maps and Search Learn about the business attributes (Opens in a Dialog Window) Close dialog window Business attributes to help people identify and support inclusive spaces on Google Maps and Search Link copied to clipboard Faye Orlove (she/her) (center), founder of Junior High, a community arts and event space that promotes the work of marginalized and underrepresented artists We’ve added features to Google Maps and Search that help people find the spaces they need, and support diversely owned businesses.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text without affecting punctuation and case\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Preserve the original case and punctuation by avoiding any further modification\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL and formatting\n",
    "def save_to_docx(cleaned_text, url, output_path='Formatted_Scraped_Content_Google.docx'):\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add a formatted title (Heading 1)\n",
    "    doc.add_heading('Scraped Content from Google Website', level=1)\n",
    "\n",
    "    # Add the URL in italic style\n",
    "    para = doc.add_paragraph()\n",
    "    run = para.add_run(f\"URL: {url}\")\n",
    "    run.italic = True\n",
    "    run.font.size = Pt(10)\n",
    "    para.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "\n",
    "    # Add a subtitle\n",
    "    doc.add_heading('Content Summary', level=2)\n",
    "\n",
    "    # Add the cleaned text with custom formatting\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(para)\n",
    "        run.font.size = Pt(11)\n",
    "        run.font.name = 'Arial'\n",
    "        p.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n",
    "\n",
    "    # Set font for the entire document\n",
    "    set_document_font(doc, font_name=\"Arial\", font_size=11)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Formatted content saved to {output_path}\")\n",
    "\n",
    "# Helper function to set the font for the entire document\n",
    "def set_document_font(document, font_name=\"Arial\", font_size=11):\n",
    "    \"\"\"Applies font and size to all paragraphs in a document.\"\"\"\n",
    "    for paragraph in document.paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            run.font.name = font_name\n",
    "            run.font.size = Pt(font_size)\n",
    "\n",
    "# Function to clean text for PDF by removing problematic Unicode characters\n",
    "def clean_text_for_pdf(text):\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://about.google/belonging/in-products/\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new formatted DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Formatted_Scraped_Content_Google.docx')\n",
    "\n",
    "        # Step 5: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b1fbfd-3d16-4749-8237-a5d404d3f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted content saved to Formatted_Scraped_Content_Google.docx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to generate a summary? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " Designing Inclusive Products for Everyone — Google Belonging Jump to content Belonging Overview At work In products In products What we’re doing Accessibility in our products In society Key issues Key issues Disability inclusion Gender equity LGBTQ+ inclusion Racial equity Veteran inclusion Resources Resources Build inclusive products Build accessible technology Create inclusive marketing Diversity report About Google Our mission, products, and impact More about our core commitments Belonging Expanding what’s possible for everyone Learning Unlocking opportunity with education & career tools Safety Center Keeping billions of people safe online Crisis Response Helping people with information in critical moments Sustainability Committed to being carbon free by 2030 Diversity report Helpful technology enables everyone to pursue their goals. Building tools that see people fairly, the power that comes from that is that people dream to be the biggest versions of themselves that they can imagine.” Link copied to clipboard Photography by Devyn Galindo More ways we’re building belonging in our products: Expanding accessible learning with Google for Education Celebrate Native American artists in Chrome and ChromeOS “Lift as you lead”: Meet 2 women defining responsible AI 4 ways Google Assistant helps me manage sensory overload Expanding accessible learning with Google for Education Celebrate Native American artists in Chrome and ChromeOS “Lift as you lead”: Meet 2 women defining responsible AI 4 ways Google Assistant helps me manage sensory overload Read more belonging stories on our blog (Opens in a New Browser Tab) Explore more of our belonging work We’re sharing tools to help creative technologists build truly accessible websites and apps. Learn more (Opens in a New Browser Tab) Watch the film 2:49 Close dialog window How we worked with the disability community to improve our speech recognition technology Watch the film 2:54 Close dialog window How our employee resource groups brought an inclusive lens to a core product Help us improve our products by getting involved in UX research (Opens in a New Browser Tab) Google employee’s quote Unmute Mute Play audio and video Play video (audio currently muted) Pause Dimitri Kanevsky (he/him) Research Scientist, Google Read the transcript (Opens in a Dialog Window) Close dialog window “I was born in Russia. Features to help people identify and support inclusive spaces on Google Maps and Search Learn about the business attributes (Opens in a Dialog Window) Close dialog window Business attributes to help people identify and support inclusive spaces on Google Maps and Search Link copied to clipboard Faye Orlove (she/her) (center), founder of Junior High, a community arts and event space that promotes the work of marginalized and underrepresented artists We’ve added features to Google Maps and Search that help people find the spaces they need, and support diversely owned businesses. Features to help people identify and support inclusive spaces on Google Maps and Search Learn about the business attributes (Opens in a Dialog Window) Close dialog window Business attributes to help people identify and support inclusive spaces on Google Maps and Search Link copied to clipboard Faye Orlove (she/her) (center), founder of Junior High, a community arts and event space that promotes the work of marginalized and underrepresented artists We’ve added features to Google Maps and Search that help people find the spaces they need, and support diversely owned businesses.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text without affecting punctuation and case\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Preserve the original case and punctuation by avoiding any further modification\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL and formatting\n",
    "def save_to_docx(cleaned_text, url, output_path='Formatted_Scraped_Content_Google.docx'):\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add a formatted title (Heading 1)\n",
    "    doc.add_heading('Scraped Content from Google Website', level=1)\n",
    "\n",
    "    # Add the URL in italic style\n",
    "    para = doc.add_paragraph()\n",
    "    run = para.add_run(f\"URL: {url}\")\n",
    "    run.italic = True\n",
    "    run.font.size = Pt(10)\n",
    "    para.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "\n",
    "    # Add a subtitle\n",
    "    doc.add_heading('Content Summary', level=2)\n",
    "\n",
    "    # Add the cleaned text with custom formatting\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(para)\n",
    "        run.font.size = Pt(11)\n",
    "        run.font.name = 'Arial'\n",
    "        p.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n",
    "\n",
    "    # Set font for the entire document\n",
    "    set_document_font(doc, font_name=\"Arial\", font_size=11)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Formatted content saved to {output_path}\")\n",
    "\n",
    "# Helper function to set the font for the entire document\n",
    "def set_document_font(document, font_name=\"Arial\", font_size=11):\n",
    "    \"\"\"Applies font and size to all paragraphs in a document.\"\"\"\n",
    "    for paragraph in document.paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            run.font.name = font_name\n",
    "            run.font.size = Pt(font_size)\n",
    "\n",
    "# Function to clean text for PDF by removing problematic Unicode characters\n",
    "def clean_text_for_pdf(text):\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://about.google/belonging/in-products/\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new formatted DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Formatted_Scraped_Content_Google.docx')\n",
    "\n",
    "        # Step 5: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6898d-6d23-4fbe-9bb0-c1432cf268ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
